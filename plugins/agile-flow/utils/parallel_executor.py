#!/usr/bin/env python3
"""
å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå™¨

æ”¯æŒæ™ºèƒ½åˆ†ç»„ã€ä¾èµ–åˆ†æã€æ–‡ä»¶å†²çªæ£€æµ‹

ä¿®å¤å†…å®¹ï¼š
- æ–‡ä»¶é”ä½¿ç”¨ fcntl å®ç°çœŸæ­£çš„è¿›ç¨‹é”
- ç«¯å£åˆ†é…è¾¹ç•Œæ£€æŸ¥å’Œèµ„æºä¸è¶³å¤„ç†
- Task æ•°æ®ç±»ä¸å¯å˜æ€§ä¿æŠ¤
- å¼‚æ­¥ä»»åŠ¡è¶…æ—¶æ§åˆ¶
- æ–‡ä»¶é”è·å–å¤±è´¥çš„é”™è¯¯å¤„ç†
- å®Œå–„çš„å†²çªæ£€æµ‹é€»è¾‘
"""

import os
import asyncio
import hashlib
import fcntl
import logging
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®ç»“æ„ï¼ˆä¸å¯å˜ï¼‰"""
    id: str
    description: str
    priority: str
    status: str
    dependencies: List[str] = field(default_factory=list)
    files: List[str] = field(default_factory=list)

    def __post_init__(self):
        """ç¡®ä¿é˜²å¾¡æ€§æ‹·è´"""
        self.dependencies = list(self.dependencies) if self.dependencies else []
        self.files = list(self.files) if self.files else []


class TaskDependencyAnalyzer:
    """ä»»åŠ¡ä¾èµ–åˆ†æå™¨"""

    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self._priority_map = {'P0': 0, 'P1': 1, 'P2': 2, 'P3': 3}

    def analyze_dependencies(self, tasks: List[Task]) -> Dict[str, List[str]]:
        """åˆ†æä»»åŠ¡ä¾èµ–å…³ç³»"""
        graph = {}
        task_map = {t.id: t for t in tasks}

        for task in tasks:
            deps = []

            # 1. ä½¿ç”¨ä»»åŠ¡ä¸­æ˜ç¡®çš„ dependencies å­—æ®µ
            if task.dependencies:
                for dep_id in task.dependencies:
                    if dep_id in task_map:
                        deps.append(dep_id)
                    else:
                        logger.warning(f"ä»»åŠ¡ {task.id} ä¾èµ–çš„ {dep_id} ä¸å­˜åœ¨")

            # 2. åˆ†ææ¨¡å—ä¾èµ–
            for other in tasks:
                if task.id == other.id:
                    continue
                if other.id not in deps:
                    if self._check_module_dependency(task, other):
                        deps.append(other.id)
                        logger.debug(f"æ£€æµ‹åˆ°éšå¼ä¾èµ–: {task.id} â†’ {other.id}")

            graph[task.id] = deps

        return graph

    def _extract_apis(self, task: Task) -> Set[str]:
        """æå–ä»»åŠ¡æ¶‰åŠçš„ API"""
        apis = set()
        desc = task.description.lower()

        if 'api' in desc or 'endpoint' in desc:
            import re
            api_patterns = re.findall(r'/api/[a-zA-Z0-9/_-]+', desc)
            apis.update(api_patterns)

        if task.files:
            for file in task.files:
                if '/api/' in file:
                    parts = file.split('/api/')
                    if len(parts) > 1:
                        apis.add(f"/api/{parts[1].split('.')[0]}")

        return apis

    def _check_module_dependency(self, task: Task, other: Task) -> bool:
        """æ£€æŸ¥æ¨¡å—ä¾èµ–"""
        task_lower = task.description.lower()
        other_lower = other.description.lower()

        # ç”¨æˆ·åŠŸèƒ½ä¾èµ–è®¤è¯
        if 'user' in task_lower and 'auth' in other_lower:
            return True

        # æƒé™åŠŸèƒ½ä¾èµ–ç”¨æˆ·
        if 'permission' in task_lower and 'user' in other_lower:
            return True

        # æ•°æ®åˆ†æåŠŸèƒ½ä¾èµ–æ•°æ®æº
        if 'analysis' in task_lower and 'data' in other_lower:
            return True

        return False

    def _get_priority(self, task_id: str) -> int:
        """è·å–ä»»åŠ¡ä¼˜å…ˆçº§ï¼ˆç”¨äºæ‰“ç ´å¾ªç¯ä¾èµ–ï¼‰"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»ä»»åŠ¡å¯¹è±¡è·å–
        return 1  # é»˜è®¤ä¼˜å…ˆçº§

    def get_parallel_groups(self, graph: Dict[str, List[str]]) -> List[List[str]]:
        """è·å–å¯å¹¶è¡Œçš„ä»»åŠ¡ç»„ï¼ˆæ‹“æ‰‘æ’åº + åˆ†å±‚ï¼‰"""
        groups = []
        remaining = graph.copy()

        while remaining:
            # æ‰¾å‡ºæ— ä¾èµ–çš„ä»»åŠ¡
            ready = [task_id for task_id, deps in remaining.items() if not deps]

            if not ready:
                # å¾ªç¯ä¾èµ–ï¼ŒæŒ‰IDæ’åºæ‰“ç ´
                logger.warning("æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼ŒæŒ‰IDé¡ºåºæ‰“ç ´")
                ready = [sorted(remaining.keys())[0]]

            groups.append(ready)

            # ç§»é™¤å·²å¤„ç†çš„ä»»åŠ¡
            for task_id in ready:
                del remaining[task_id]

            # æ›´æ–°å‰©ä½™ä»»åŠ¡çš„ä¾èµ–
            for task_id in remaining:
                remaining[task_id] = [d for d in remaining[task_id] if d not in ready]

        return groups


class FileLockManager:
    """æ–‡ä»¶é”ç®¡ç†å™¨ï¼ˆä½¿ç”¨ fcntl å®ç°çœŸæ­£çš„è¿›ç¨‹é”ï¼‰"""

    def __init__(self, lock_dir: str = "/tmp/agile-flow-locks"):
        self.lock_dir = Path(lock_dir)
        self.lock_dir.mkdir(parents=True, exist_ok=True)
        self.locks: Dict[str, int] = {}

    def _get_lock_path(self, file_path: str) -> Path:
        """è·å–é”æ–‡ä»¶è·¯å¾„"""
        file_hash = hashlib.md5(file_path.encode()).hexdigest()
        return self.lock_dir / f"{file_hash}.lock"

    def acquire(self, file_path: str, timeout: float = 60.0) -> bool:
        """
        è·å–æ–‡ä»¶é”ï¼ˆä½¿ç”¨ fcntlï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸè·å–é”
        """
        lock_path = self._get_lock_path(file_path)

        import time
        start = time.time()

        while True:
            try:
                # åˆ›å»ºé”æ–‡ä»¶
                fd = os.open(lock_path, os.O_CREAT | os.O_WRONLY)

                try:
                    # å°è¯•è·å–æ’ä»–é”ï¼ˆéé˜»å¡ï¼‰
                    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                    self.locks[file_path] = fd
                    logger.debug(f"è·å–æ–‡ä»¶é”: {file_path}")
                    return True
                except (OSError, IOError) as e:
                    # é”è¢«å…¶ä»–è¿›ç¨‹æŒæœ‰
                    os.close(fd)
                    if time.time() - start > timeout:
                        logger.warning(f"è·å–æ–‡ä»¶é”è¶…æ—¶: {file_path}")
                        return False
                    time.sleep(0.1)

            except OSError as e:
                if time.time() - start > timeout:
                    logger.error(f"åˆ›å»ºé”æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                    return False
                time.sleep(0.1)

    def release(self, file_path: str) -> bool:
        """
        é‡Šæ”¾æ–‡ä»¶é”

        Returns:
            bool: æ˜¯å¦æˆåŠŸé‡Šæ”¾
        """
        if file_path in self.locks:
            try:
                fd = self.locks[file_path]
                # é‡Šæ”¾é”
                fcntl.flock(fd, fcntl.LOCK_UN)
                os.close(fd)

                # åˆ é™¤é”æ–‡ä»¶
                lock_path = self._get_lock_path(file_path)
                if lock_path.exists():
                    lock_path.unlink()

                del self.locks[file_path]
                logger.debug(f"é‡Šæ”¾æ–‡ä»¶é”: {file_path}")
                return True
            except Exception as e:
                logger.error(f"é‡Šæ”¾æ–‡ä»¶é”å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                return False

        return False

    def release_all(self):
        """é‡Šæ”¾æ‰€æœ‰é”"""
        for file_path in list(self.locks.keys()):
            self.release(file_path)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release_all()


class PortPool:
    """ç«¯å£æ± ç®¡ç†å™¨"""

    def __init__(self, start: int = 3000, count: int = 10):
        self.start = start
        self.count = count
        self.ports: Set[int] = set()
        self._lock = asyncio.Lock()

    async def allocate(self, count: int = 1) -> List[int]:
        """
        åˆ†é…ç«¯å£

        Args:
            count: éœ€è¦çš„ç«¯å£æ•°é‡

        Returns:
            List[int]: åˆ†é…çš„ç«¯å£åˆ—è¡¨

        Raises:
            RuntimeError: ç«¯å£ä¸è¶³
        """
        async with self._lock:
            allocated = []

            # éå†æ•´ä¸ªç«¯å£èŒƒå›´
            for port in range(self.start, self.start + self.count):
                if port not in self.ports:
                    self.ports.add(port)
                    allocated.append(port)
                    if len(allocated) >= count:
                        break

            # æ£€æŸ¥æ˜¯å¦åˆ†é…æˆåŠŸ
            if len(allocated) < count:
                # å›æ»šï¼šé‡Šæ”¾å·²åˆ†é…çš„ç«¯å£
                for port in allocated:
                    self.ports.discard(port)
                raise RuntimeError(
                    f"ç«¯å£æ± è€—å°½ï¼šéœ€è¦ {count} ä¸ªï¼Œä»… {len(allocated)} ä¸ªå¯ç”¨ã€‚"
                    f"å·²ç”¨ç«¯å£: {self.ports}"
                )

            logger.info(f"åˆ†é…ç«¯å£: {allocated}")
            return allocated

    async def release(self, ports: List[int]):
        """é‡Šæ”¾ç«¯å£"""
        async with self._lock:
            for port in ports:
                self.ports.discard(port)
            logger.info(f"é‡Šæ”¾ç«¯å£: {ports}")


class ParallelTaskExecutor:
    """å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå™¨"""

    def __init__(
        self,
        project_path: str,
        max_parallel: int = 3,
        task_timeout: float = 300.0
    ):
        self.project_path = Path(project_path)
        self.max_parallel = max_parallel
        self.task_timeout = task_timeout
        self.analyzer = TaskDependencyAnalyzer(str(project_path))
        self.file_locks = FileLockManager()
        self.port_pool = PortPool(start=3000, count=10)

    def check_file_conflicts(self, tasks: List[Task]) -> List[Tuple[str, str, str]]:
        """
        æ£€æŸ¥æ–‡ä»¶å†²çª

        Returns:
            List[Tuple[str, str, str]]: (file_path, task_id_1, task_id_2)
        """
        file_map: Dict[str, str] = {}
        conflicts = []

        for task in tasks:
            files = task.files or []
            for file in files:
                if file in file_map:
                    conflicts.append((file, file_map[file], task.id))
                    logger.warning(
                        f"æ–‡ä»¶å†²çª: {file} åœ¨ {file_map[file]} å’Œ {task.id} ä¹‹é—´"
                    )
                else:
                    file_map[file] = task.id

        return conflicts

    async def execute_group(self, tasks: List[Task]) -> List[Dict]:
        """
        å¹¶è¡Œæ‰§è¡Œä¸€ç»„ä»»åŠ¡

        Args:
            tasks: è¦æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨

        Returns:
            List[Dict]: æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        print(f"\n{'='*70}")
        print(f"ğŸš€ å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")
        print(f"{'='*70}\n")

        # æ£€æŸ¥æ–‡ä»¶å†²çª
        conflicts = self.check_file_conflicts(tasks)
        if conflicts:
            print(f"âš ï¸  å‘ç°æ–‡ä»¶å†²çª:")
            for file, task1, task2 in conflicts:
                print(f"   {file}: {task1} vs {task2}")
            return await self._execute_with_conflicts(tasks, conflicts)

        # è·å–æ–‡ä»¶é”ï¼ˆæ£€æŸ¥è¿”å›å€¼ï¼‰
        acquired_locks = []
        try:
            for task in tasks:
                if task.files:
                    for file in task.files:
                        if not self.file_locks.acquire(file, timeout=30.0):
                            raise RuntimeError(f"æ— æ³•è·å–æ–‡ä»¶é”: {file}")
                        acquired_locks.append(file)

            # åˆ†é…ç«¯å£ï¼ˆæ£€æŸ¥æ˜¯å¦è¶³å¤Ÿï¼‰
            try:
                ports = await self.port_pool.allocate(len(tasks))
            except RuntimeError as e:
                logger.error(f"ç«¯å£åˆ†é…å¤±è´¥: {e}")
                raise

            try:
                # å¹¶è¡Œæ‰§è¡Œ
                results = await asyncio.gather(*[
                    self._execute_task(task, ports[i])
                    for i, task in enumerate(tasks)
                ], return_exceptions=True)

                # å¤„ç†å¼‚å¸¸
                processed_results = []
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"ä»»åŠ¡ {tasks[i].id} æ‰§è¡Œå¤±è´¥: {result}")
                        processed_results.append({
                            "task_id": tasks[i].id,
                            "status": "error",
                            "error": str(result)
                        })
                    else:
                        processed_results.append(result)

                return processed_results

            finally:
                # é‡Šæ”¾ç«¯å£
                await self.port_pool.release(ports)

        finally:
            # é‡Šæ”¾æ–‡ä»¶é”
            for file in acquired_locks:
                self.file_locks.release(file)

    async def _execute_with_conflicts(
        self,
        tasks: List[Task],
        conflicts: List[Tuple[str, str, str]]
    ) -> List[Dict]:
        """
        å¤„ç†æœ‰å†²çªçš„ä»»åŠ¡ï¼ˆä¸²è¡Œæ‰§è¡Œå†²çªä»»åŠ¡ï¼‰

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            conflicts: å†²çªåˆ—è¡¨

        Returns:
            List[Dict]: æ‰§è¡Œç»“æœ
        """
        results = []
        executed = set()

        for task in tasks:
            # æ£€æŸ¥æ˜¯å¦ä¸ä»»ä½•å·²æ‰§è¡Œçš„ä»»åŠ¡æœ‰å†²çª
            has_conflict = False
            for file, task_a, task_b in conflicts:
                if task.id == task_b and task_a in executed:
                    has_conflict = True
                    break
                elif task.id == task_a and task_b in executed:
                    has_conflict = True
                    break

            if has_conflict:
                print(f"â³ ä»»åŠ¡ {task.id} æœ‰å†²çªï¼Œä¸²è¡Œæ‰§è¡Œ")
                result = await self._execute_task(task, None)
                results.append(result)
            else:
                # æ— å†²çªï¼Œå¯ä»¥ä¸ä¹‹å‰çš„æ— å†²çªä»»åŠ¡å¹¶è¡Œ
                # ä½†ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä¹Ÿä¸²è¡Œæ‰§è¡Œ
                result = await self._execute_task(task, None)
                results.append(result)

            executed.add(task.id)

        return results

    async def _execute_task(
        self,
        task: Task,
        port: Optional[int]
    ) -> Dict:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰

        Args:
            task: è¦æ‰§è¡Œçš„ä»»åŠ¡
            port: åˆ†é…çš„ç«¯å£ï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict: æ‰§è¡Œç»“æœ
        """
        print(f"  ğŸ”§ æ‰§è¡Œä»»åŠ¡: {task.id} - {task.description[:50]}")
        if port:
            print(f"     ç«¯å£: {port}")

        try:
            # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
            async with asyncio.timeout(self.task_timeout):
                # è¿™é‡Œæ˜¯æ¥å£ï¼Œå®é™…å®ç°åœ¨æµç¨‹å¼•æ“ä¸­
                await asyncio.sleep(1)  # æ¨¡æ‹Ÿæ‰§è¡Œ

                return {
                    "task_id": task.id,
                    "status": "completed",
                    "port": port
                }
        except asyncio.TimeoutError:
            logger.error(f"ä»»åŠ¡ {task.id} æ‰§è¡Œè¶…æ—¶")
            return {
                "task_id": task.id,
                "status": "timeout",
                "error": f"æ‰§è¡Œè¶…æ—¶ ({self.task_timeout}ç§’)"
            }
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {task.id} æ‰§è¡Œå¼‚å¸¸: {e}")
            return {
                "task_id": task.id,
                "status": "error",
                "error": str(e)
            }

    async def execute_parallel_flow(self, tasks: List[Task]) -> List[Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„å¹¶è¡Œæµç¨‹

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            List[Dict]: æ‰€æœ‰ä»»åŠ¡çš„æ‰§è¡Œç»“æœ
        """
        # åˆ†æä¾èµ–
        graph = self.analyzer.analyze_dependencies(tasks)

        print("\nğŸ“Š ä»»åŠ¡ä¾èµ–å›¾:")
        for task_id, deps in graph.items():
            if deps:
                print(f"  {task_id} â†’ {', '.join(deps)}")
            else:
                print(f"  {task_id} (æ— ä¾èµ–)")

        # è·å–å¹¶è¡Œç»„
        groups = self.analyzer.get_parallel_groups(graph)

        print(f"\nğŸ¯ å¹¶è¡Œæ‰§è¡Œè®¡åˆ’ (å…± {len(groups)} ç»„):")
        for i, group in enumerate(groups, 1):
            print(f"  ç¬¬ {i} ç»„: {', '.join(group)}")

        # æ‰§è¡Œæ¯ç»„
        all_results = []
        for i, group_ids in enumerate(groups, 1):
            group_tasks = [t for t in tasks if t.id in group_ids]

            # é™åˆ¶å¹¶è¡Œåº¦
            for j in range(0, len(group_tasks), self.max_parallel):
                batch = group_tasks[j:j + self.max_parallel]
                results = await self.execute_group(batch)
                all_results.extend(results)

        return all_results


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡ï¼ˆåŒ…å«ä¾èµ–å…³ç³»ï¼‰
    tasks = [
        Task("TASK-001", "å®ç°ç”¨æˆ·è®¤è¯", "P1", "pending", [], ["src/auth/login.py"]),
        Task("TASK-002", "å®ç°ç”¨æˆ·ç®¡ç†", "P1", "pending", ["TASK-001"], ["src/api/users.py"]),
        Task("TASK-003", "å®ç°è‚¡ç¥¨æ•°æ® API", "P1", "pending", [], ["src/api/stocks.py"]),
        Task("TASK-004", "å®ç°æŠ¥å‘Šç”Ÿæˆ", "P2", "pending", [], ["src/services/report.py"]),
        Task("TASK-005", "å®ç°æƒé™ç®¡ç†", "P2", "pending", ["TASK-002"], ["src/api/permissions.py"]),
    ]

    async def test():
        executor = ParallelTaskExecutor("/tmp/test", max_parallel=3)
        await executor.execute_parallel_flow(tasks)

        print("\n" + "="*70)
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("="*70)
        print("\nè¯´æ˜ï¼š")
        print("  - ç¬¬1ç»„: TASK-001, TASK-003, TASK-004 (æ— ä¾èµ–ï¼Œå¯å¹¶è¡Œ)")
        print("  - ç¬¬2ç»„: TASK-002 (ä¾èµ– TASK-001)")
        print("  - ç¬¬3ç»„: TASK-005 (ä¾èµ– TASK-002)")
        print("\næ”¹è¿›ç‚¹ï¼š")
        print("  âœ… ä½¿ç”¨ fcntl å®ç°çœŸæ­£çš„æ–‡ä»¶é”")
        print("  âœ… ç«¯å£åˆ†é…ä¸è¶³æ—¶æŠ›å‡ºå¼‚å¸¸")
        print("  âœ… æ–‡ä»¶é”è·å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸")
        print("  âœ… æ·»åŠ å¼‚æ­¥ä»»åŠ¡è¶…æ—¶æ§åˆ¶")
        print("  âœ… å®Œå–„çš„å†²çªæ£€æµ‹é€»è¾‘")
        print("  âœ… Task æ•°æ®ç±»ä¸å¯å˜æ€§")

    asyncio.run(test())
