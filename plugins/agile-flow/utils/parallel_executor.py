#!/usr/bin/env python3
"""
å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå™¨

æ”¯æŒæ™ºèƒ½åˆ†ç»„ã€ä¾èµ–åˆ†æã€æ–‡ä»¶å†²çªæ£€æµ‹
"""

import os
import json
import asyncio
import hashlib
import fcntl
from pathlib import Path
from typing import Dict, List, Set, Tuple
from dataclasses import dataclass


@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®ç»“æ„"""
    id: str
    description: str
    priority: str
    status: str
    dependencies: List[str] = None
    files: List[str] = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.files is None:
            self.files = []


class TaskDependencyAnalyzer:
    """ä»»åŠ¡ä¾èµ–åˆ†æå™¨"""

    def __init__(self, project_path: str):
        self.project_path = Path(project_path)

    def analyze_dependencies(self, tasks: List[Task]) -> Dict[str, List[str]]:
        """åˆ†æä»»åŠ¡ä¾èµ–å…³ç³»"""
        graph = {}

        # å…ˆæ„å»º ID åˆ°ä»»åŠ¡çš„æ˜ å°„
        task_map = {t.id: t for t in tasks}

        for task in tasks:
            deps = []

            # 1. ä½¿ç”¨ä»»åŠ¡ä¸­æ˜ç¡®çš„ dependencies å­—æ®µ
            if task.dependencies:
                for dep_id in task.dependencies:
                    if dep_id in task_map:
                        deps.append(dep_id)

            # 2. åˆ†ææ¨¡å—ä¾èµ–ï¼ˆç”¨æˆ·ä»»åŠ¡ä¾èµ–è®¤è¯ä»»åŠ¡ç­‰ï¼‰
            for other in tasks:
                if task.id == other.id:
                    continue

                # å¦‚æœä¸åœ¨æ˜ç¡®ä¾èµ–ä¸­ï¼Œæ£€æŸ¥éšå¼ä¾èµ–
                if other.id not in deps:
                    if self._check_module_dependency(task, other):
                        deps.append(other.id)

            graph[task.id] = deps

        return graph

    def _extract_apis(self, task: Task) -> Set[str]:
        """æå–ä»»åŠ¡æ¶‰åŠçš„ API"""
        apis = set()
        desc = task.description.lower()

        # å¸¸è§ API æ¨¡å¼
        if 'api' in desc or 'endpoint' in desc:
            # æå– /api/xxx æ¨¡å¼
            import re
            api_patterns = re.findall(r'/api/[a-zA-Z0-9/_-]+', desc)
            apis.update(api_patterns)

        # æ£€æŸ¥ä»»åŠ¡æ ‡ç­¾
        if task.files:
            for file in task.files:
                if '/api/' in file:
                    # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­ API
                    parts = file.split('/api/')
                    if len(parts) > 1:
                        apis.add(f"/api/{parts[1].split('.')[0]}")

        return apis

    def _extract_modules(self, task: Task) -> Set[str]:
        """æå–ä»»åŠ¡æ¶‰åŠçš„æ¨¡å—"""
        modules = set()
        if task.files:
            for file in task.files:
                # æå–ä¸€çº§ç›®å½•ä½œä¸ºæ¨¡å—
                parts = Path(file).parts
                if len(parts) > 1:
                    modules.add(parts[0])
        return modules

    def _check_module_dependency(self, task: Task, other: Task) -> bool:
        """æ£€æŸ¥æ¨¡å—ä¾èµ–"""
        # å¸¸è§ä¾èµ–è§„åˆ™
        task_lower = task.description.lower()
        other_lower = other.description.lower()

        # ç”¨æˆ·åŠŸèƒ½ä¾èµ–è®¤è¯
        if 'user' in task_lower and 'auth' in other_lower:
            return True

        # æƒé™åŠŸèƒ½ä¾èµ–ç”¨æˆ·
        if 'permission' in task_lower and 'user' in other_lower:
            return True

        # æ•°æ®åˆ†æåŠŸèƒ½ä¾èµ–æ•°æ®æº
        if 'analysis' in task_lower and 'data' in other_lower:
            return True

        return False

    def get_parallel_groups(self, graph: Dict[str, List[str]]) -> List[List[str]]:
        """è·å–å¯å¹¶è¡Œçš„ä»»åŠ¡ç»„ï¼ˆæ‹“æ‰‘æ’åº + åˆ†å±‚ï¼‰"""
        groups = []
        remaining = graph.copy()

        while remaining:
            # æ‰¾å‡ºæ— ä¾èµ–çš„ä»»åŠ¡
            ready = [task_id for task_id, deps in remaining.items() if not deps]

            if not ready:
                # å¾ªç¯ä¾èµ–ï¼ŒæŒ‰ä¼˜å…ˆçº§æ‰“ç ´
                ready = [min(remaining.keys(), key=lambda x: self._get_priority(x))]

            groups.append(ready)

            # ç§»é™¤å·²å¤„ç†çš„ä»»åŠ¡
            for task_id in ready:
                del remaining[task_id]

            # æ›´æ–°å‰©ä½™ä»»åŠ¡çš„ä¾èµ–
            for task_id in remaining:
                remaining[task_id] = [d for d in remaining[task_id] if d not in ready]

        return groups


class FileLockManager:
    """æ–‡ä»¶é”ç®¡ç†å™¨"""

    def __init__(self, lock_dir: str = "/tmp/agile-flow-locks"):
        self.lock_dir = Path(lock_dir)
        self.lock_dir.mkdir(parents=True, exist_ok=True)
        self.locks: Dict[str, int] = {}

    def _get_lock_path(self, file_path: str) -> Path:
        """è·å–é”æ–‡ä»¶è·¯å¾„"""
        file_hash = hashlib.md5(file_path.encode()).hexdigest()
        return self.lock_dir / f"{file_hash}.lock"

    def acquire(self, file_path: str, timeout: float = 60.0) -> bool:
        """è·å–æ–‡ä»¶é”"""
        lock_path = self._get_lock_path(file_path)

        import time
        start = time.time()

        while True:
            try:
                fd = os.open(lock_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                self.locks[file_path] = fd
                return True
            except FileExistsError:
                if time.time() - start > timeout:
                    return False
                time.sleep(0.5)

    def release(self, file_path: str):
        """é‡Šæ”¾æ–‡ä»¶é”"""
        if file_path in self.locks:
            os.close(self.locks[file_path])
            del self.locks[file_path]

        lock_path = self._get_lock_path(file_path)
        if lock_path.exists():
            lock_path.unlink()

    def release_all(self):
        """é‡Šæ”¾æ‰€æœ‰é”"""
        for file_path in list(self.locks.keys()):
            self.release(file_path)


class PortPool:
    """ç«¯å£æ± ç®¡ç†å™¨"""

    def __init__(self, start: int = 3000, count: int = 10):
        self.start = start
        self.count = count
        self.ports: Set[int] = set()

    def allocate(self, count: int = 1) -> List[int]:
        """åˆ†é…ç«¯å£"""
        allocated = []
        for i in range(self.count):
            port = self.start + i
            if port not in self.ports:
                self.ports.add(port)
                allocated.append(port)
                if len(allocated) >= count:
                    break
        return allocated

    def release(self, ports: List[int]):
        """é‡Šæ”¾ç«¯å£"""
        for port in ports:
            self.ports.discard(port)


class ParallelTaskExecutor:
    """å¹¶è¡Œä»»åŠ¡æ‰§è¡Œå™¨"""

    def __init__(self, project_path: str, max_parallel: int = 3):
        self.project_path = Path(project_path)
        self.max_parallel = max_parallel
        self.analyzer = TaskDependencyAnalyzer(str(project_path))
        self.file_locks = FileLockManager()
        self.port_pool = PortPool(start=3000, count=10)

    def check_file_conflicts(self, tasks: List[Task]) -> List[Tuple[str, str, str]]:
        """æ£€æŸ¥æ–‡ä»¶å†²çª"""
        file_map: Dict[str, str] = {}
        conflicts = []

        for task in tasks:
            files = task.files or []
            for file in files:
                if file in file_map:
                    conflicts.append((file, file_map[file], task.id))
                else:
                    file_map[file] = task.id

        return conflicts

    async def execute_group(self, tasks: List[Task]) -> List[Dict]:
        """å¹¶è¡Œæ‰§è¡Œä¸€ç»„ä»»åŠ¡"""
        print(f"\n{'='*70}")
        print(f"ğŸš€ å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")
        print(f"{'='*70}\n")

        # æ£€æŸ¥æ–‡ä»¶å†²çª
        conflicts = self.check_file_conflicts(tasks)
        if conflicts:
            print(f"âš ï¸  å‘ç°æ–‡ä»¶å†²çª:")
            for file, task1, task2 in conflicts:
                print(f"   {file}: {task1} vs {task2}")
            # å¯ä»¥é€‰æ‹©ï¼š1) ä¸²è¡Œæ‰§è¡Œ 2) é‡æ–°åˆ†ç»„ 3) æŠ¥é”™
            # è¿™é‡Œé€‰æ‹©ä¸²è¡Œæ‰§è¡Œæœ‰å†²çªçš„ä»»åŠ¡
            return await self._execute_with_conflicts(tasks, conflicts)

        # åˆ†é…ç«¯å£
        ports = self.port_pool.allocate(len(tasks))

        # è·å–æ–‡ä»¶é”
        for task in tasks:
            if task.files:
                for file in task.files:
                    self.file_locks.acquire(file)

        try:
            # å¹¶è¡Œæ‰§è¡Œ
            results = await asyncio.gather(*[
                self._execute_task(task, ports[i] if i < len(ports) else None)
                for i, task in enumerate(tasks)
            ])
            return results
        finally:
            # é‡Šæ”¾é”
            for task in tasks:
                if task.files:
                    for file in task.files:
                        self.file_locks.release(file)
            # é‡Šæ”¾ç«¯å£
            self.port_pool.release(ports)

    async def _execute_with_conflicts(self, tasks: List[Task], conflicts: List) -> List[Dict]:
        """å¤„ç†æœ‰å†²çªçš„ä»»åŠ¡"""
        # ç®€å•ç­–ç•¥ï¼šä¸²è¡Œæ‰§è¡Œæœ‰å†²çªçš„ä»»åŠ¡
        results = []
        executed = set()

        for task in tasks:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æ‰§è¡Œçš„ä»»åŠ¡å†²çª
            has_conflict = False
            for file, task1, task2 in conflicts:
                if task.id == task2 and task1 in executed:
                    has_conflict = True
                    break

            if has_conflict:
                print(f"â³ ä»»åŠ¡ {task.id} æœ‰å†²çªï¼Œä¸²è¡Œæ‰§è¡Œ")
                # ä¸²è¡Œæ‰§è¡Œï¼ˆç­‰å¾…ï¼‰
                result = await self._execute_task(task, None)
                results.append(result)
            else:
                results.append(None)  # å ä½

            executed.add(task.id)

        return results

    async def _execute_task(self, task: Task, port: int = None) -> Dict:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        # è¿™é‡Œæ˜¯æ¥å£ï¼Œå®é™…å®ç°åœ¨æµç¨‹å¼•æ“ä¸­
        print(f"  ğŸ”§ æ‰§è¡Œä»»åŠ¡: {task.id} - {task.description[:50]}")
        if port:
            print(f"     ç«¯å£: {port}")

        # æ¨¡æ‹Ÿæ‰§è¡Œ
        await asyncio.sleep(1)

        return {
            "task_id": task.id,
            "status": "completed",
            "port": port
        }

    async def execute_parallel_flow(self, tasks: List[Task]):
        """æ‰§è¡Œå®Œæ•´çš„å¹¶è¡Œæµç¨‹"""
        # åˆ†æä¾èµ–
        graph = self.analyzer.analyze_dependencies(tasks)

        print("\nğŸ“Š ä»»åŠ¡ä¾èµ–å›¾:")
        for task_id, deps in graph.items():
            if deps:
                print(f"  {task_id} â†’ {', '.join(deps)}")
            else:
                print(f"  {task_id} (æ— ä¾èµ–)")

        # è·å–å¹¶è¡Œç»„
        groups = self.analyzer.get_parallel_groups(graph)

        print(f"\nğŸ¯ å¹¶è¡Œæ‰§è¡Œè®¡åˆ’ (å…± {len(groups)} ç»„):")
        for i, group in enumerate(groups, 1):
            print(f"  ç¬¬ {i} ç»„: {', '.join(group)}")

        # æ‰§è¡Œæ¯ç»„
        all_results = []
        for i, group_ids in enumerate(groups, 1):
            group_tasks = [t for t in tasks if t.id in group_ids]

            # é™åˆ¶å¹¶è¡Œåº¦
            for j in range(0, len(group_tasks), self.max_parallel):
                batch = group_tasks[j:j + self.max_parallel]
                results = await self.execute_group(batch)
                all_results.extend(results)

        return all_results


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡ï¼ˆåŒ…å«ä¾èµ–å…³ç³»ï¼‰
    tasks = [
        Task("TASK-001", "å®ç°ç”¨æˆ·è®¤è¯", "P1", "pending", [], ["src/auth/login.py"]),
        Task("TASK-002", "å®ç°ç”¨æˆ·ç®¡ç†", "P1", "pending", ["TASK-001"], ["src/api/users.py"]),
        Task("TASK-003", "å®ç°è‚¡ç¥¨æ•°æ® API", "P1", "pending", [], ["src/api/stocks.py"]),
        Task("TASK-004", "å®ç°æŠ¥å‘Šç”Ÿæˆ", "P2", "pending", [], ["src/services/report.py"]),
        Task("TASK-005", "å®ç°æƒé™ç®¡ç†", "P2", "pending", ["TASK-002"], ["src/api/permissions.py"]),
    ]

    async def test():
        executor = ParallelTaskExecutor("/tmp/test", max_parallel=3)
        await executor.execute_parallel_flow(tasks)

        print("\n" + "="*70)
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("="*70)
        print("\nè¯´æ˜ï¼š")
        print("  - ç¬¬1ç»„: TASK-001, TASK-003, TASK-004 (æ— ä¾èµ–ï¼Œå¯å¹¶è¡Œ)")
        print("  - ç¬¬2ç»„: TASK-002 (ä¾èµ– TASK-001)")
        print("  - ç¬¬3ç»„: TASK-005 (ä¾èµ– TASK-002)")

    asyncio.run(test())
